{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAT_DAML_DAY04_03ConvolutionNN_TensorFlowGG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IQnIfUdleSo7",
        "K058Um_tsK2m",
        "99h6Q-fNwe5U",
        "56P2AcBL231S",
        "fUOZWP_e3BzX",
        "XisUFm5-3RCl",
        "R6NRNM8T3dob"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQBgWudQpDxv",
        "colab_type": "text"
      },
      "source": [
        "<table ><tr><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://web.facebook.com/DAT.KUSRC/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1dNBiKikzW1-osi6lleLOgSOKQ65IIfMC\" height=\"50px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://www.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ZfGOBmxAwg8SAhyseFziyinzxBGme78a\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "<a href=\"https://www.tensorflow.org/\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/1200px-TensorFlowLogo.svg.png\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://mike.cpe.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1s6r3iG_Slpu_NSWqdt5zBp8Z9hV0-zh6\" height=\"50px\"></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fsj-W926IGN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "<center><h1><b>Introduction to Convolutional NN</b></h1></center>\n",
        "\n",
        "---\n",
        "*   Acknowledgement: Most parts of this tutorail were extracted from [Machine Learning, Zero to Hero](https://www.youtube.com/watch?v=x_VrgWTKkiM&t=11s), Google TensorFlow 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa-R1jfh9qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Introduction to Convolutional NN..')\n",
        "print('  Brought to you by K.Toto@MikeLab.Net')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnIfUdleSo7",
        "colab_type": "text"
      },
      "source": [
        "## 1. Idea behind the convolutional NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4zPfLOR-pe",
        "colab_type": "text"
      },
      "source": [
        "In the previous tutorial, we have seen how to do basic computer vision using a deep neural network that matched the pixels of an image to a label. So the image like this was matched to a numeric label that represent the object of class <font color=ff00ff>'*ankle boot*'</font>.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1KKQP_bnEfZ2Yu4pGR74HraitdRgSU5QV></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqWSeWYGp3Ml",
        "colab_type": "text"
      },
      "source": [
        "However, there was a limitation to that the image we were working with had to have the subject <font color=ff00ff>centered</font> in it, and it had to be the <font color=ff00ff>only thing</font> in the image. So the code we have written would just work for that shoe. But what are about these?\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1uSv7tLTsmEm7Brd9VfwaFc9co0p9Zp8V></center>\n",
        "\n",
        "It wouldn't be able to identify all of them because <font color=ff00ff>it was not trained to do so</font>. For that we have to use something called a <font color=00ffff>**convolution neural network**</font>, which works a little differently than what we have seen. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLIh3zrsUMfa",
        "colab_type": "text"
      },
      "source": [
        "The *idea behind* a convolutional network is that we will <font color=ff00ff>**filter**</font> the images before training the deep neural network. After filtering the images, <font color=ff00ff>**features**</font> within the images could then *come to the* <font color=00ffff>**forefront**</font> and we then can <font color=ff00ff>*spot*</font> those features to identify something.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1GcajQ1eEoYROXuNRaQCeBNfWiN62Dn5Y></center>\n",
        "\n",
        "A filter is simply a <font color=ff00ff>set of multipliers</font>. For example, in this case if we are looking at a particular pixel, that has the value 192, and the filter is the values in the red box. Then we multiply 192 by 4.5, and each of its neighbors by the respective filter value. When we sum up the result, we will get the new value for the pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iW4KDKel--E",
        "colab_type": "text"
      },
      "source": [
        "This might seem a little odd, but check out the results for filters like this one, that when multiplied over the contents of the image, it removes almost everthing except the <font color=ff00ff>*vertical lines*</font>.\n",
        "\n",
        "<center><img src=https://drive.google.com/uc?id=18ymMfOYvciLBhas4pc72vG1dfiyTXDXh></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "And for the filter like this one, it will remove almost everything except the <font color=ff00ff>*horizontal lines*</font>.\n",
        "\n",
        "<center><img src=https://drive.google.com/uc?id=1hUg8Jalh-Fs6Z7eL6u2ZfAwnMynI8AII></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvND8sm5nQod",
        "colab_type": "text"
      },
      "source": [
        "This can then be combined with something called <font color=ff00ff>**pooling**</font>, which groups up the pixels in the image and *filters* them *down* (aka. <font color=ff00ff>down sampling</font>) to a subset.\n",
        "\n",
        "For example, the <font color=ff00ff>*max pooling two by two*</font> will group the image into sets of 2x2 pixels, and simply pick the largest. The image will be reduced to a quarter of its original size, but the features can still be maintained.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1Uq7dz7M0Q7ispUVc-oAX-X2zfMZ6LYZf></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZ_t4Ofodav",
        "colab_type": "text"
      },
      "source": [
        "Therefore the previous image after being filtered and then max pooled could look like this. The image on the right is one quarter the size of the one on the left, but the vertical line features were still maintained, and indeed they were <font color=ff00ff>*enhanced*</font>.\n",
        "\n",
        "<center><img src=https://drive.google.com/uc?id=1V2Mch9DtOxv0weWsO7qIUpHhf-hf6Pjo></center>\n",
        "\n",
        "So, where did these filters come from?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfChyZ6dpnsi",
        "colab_type": "text"
      },
      "source": [
        "That is the magic of a convolutional neural network. They actually <font color=ff00ff>**learned**</font>. They are just parameters like those in the neurons of the neural network that have seen from the last tutorial. \n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=17lseIsHMyrAVCh2NrPeoohUECcbS6oww></center>\n",
        "\n",
        "As our image is fed into the <font color=ff00ff>**convolutional layer**</font>, a number of randomly initialized filters will pass over the image. The results of these are fed into the next layer, and matching is performed by the neuron network. And over time, the filters that give us the image outputs that give the best matches will be <font color=ff00ff>learned</font>, and this process is called the <font color=ff00ff>**feature extraction**</font>. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbyYhMLvrGW6",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of how a convolutional filter layer can help a computer visualize things.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1vtuQNe4b5maIbbMT8rYnK0SU2tVaawxc></center>\n",
        "\n",
        "We can see across the top row above that we actually have a shoe, but it has been filtered down to the sole and the silhouette of a shoe by filters that learned what a shoe looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K058Um_tsK2m",
        "colab_type": "text"
      },
      "source": [
        "## 2. Explain the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QSL3g_isOVr",
        "colab_type": "text"
      },
      "source": [
        "Now let's take a look at the code to build a convolutional neural network like this. This code is very similar to what we have seen earlier.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1O2F7nzJ14Wg1ITjKyzuzP7rbRo22DqHM></center>\n",
        "\n",
        "We have a <font color=ff00ff>flattened input</font> that is fed into a <font color=ff00ff>dense layer</font>, that in turn is fed into the final dense layer that is our <font color=ff00ff>output</font>. Only the difference here is that we haven't specified the <font color=ff00ff>input shape</font>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoBC8NXUtUjh",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1eBkofro5DWkk52ixcynJW0I-Fs5fYkdI></center>\n",
        "\n",
        "That is because we'll put a <font color=ff00ff>convolutional layer</font> on the top of it like this. This first layer takes the input so we specify the <font color=ff00ff>input shape</font>, and we are telling it to generate 64 filters with this parameter. That is, it will generate 64 filters and multiply each of them across the image. Then each epoch, it will <font color=ff00ff>figure out</font> which filters gave the best signals to help match the image to their labels in much the same way it <font color=ff00ff>learned</font> which parameters worked best in the dense layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPF9vkJ_u0ZJ",
        "colab_type": "text"
      },
      "source": [
        "The <font color=ff00ff>max pooling</font> to <font color=ffff00>compress</font> the image and <font color=ffff00>enhance</font> the features look like this.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1-HXu2aU88iwjsd-0D2yChz9e9uwNe7Ng></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQYwju2MvXED",
        "colab_type": "text"
      },
      "source": [
        "We can also <font color=ff00ff>stack</font> the <font color=00ffff>convolutional layers</font> on the top of each other to really <font color=ff00ff>break down</font> the image, and try to <font color=ff00ff>learn from very abstract features</font> like this.\n",
        "\n",
        "<br>\n",
        "<center><img src=https://drive.google.com/uc?id=1A7ZkNgiaTeVH7fPlgqcOZgY2LOtTSSJu></center>\n",
        "\n",
        "With this methodology, our network starts to <font color=ff00ff>*learn*</font> based on the <font color=ff00ff>*features*</font> of the image instead of just the raw patterns of pixels.\n",
        "\n",
        "Two sleeves, it's a shirt. Two short sleeves, it's a T-shirt. Sole and laces, it's a shoe -- that type of thing.\n",
        "\n",
        "Now we are still looking at just the simple image as a fashion at the moment. But the principles will extend into *more* complex images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99h6Q-fNwe5U",
        "colab_type": "text"
      },
      "source": [
        "##3 Your Exercise:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTJZ0kbywjpA",
        "colab_type": "text"
      },
      "source": [
        "Complete the fahsion mnist learning example, using the convolutional NN technique just learned from this section, and send your code here.\n",
        "\n",
        "You should also compare the learing curves of each epoch between using or not using the convolutional NN technique.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56P2AcBL231S",
        "colab_type": "text"
      },
      "source": [
        "### Load the fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSfnLvtswyEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the FashionMnist using keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "print('Done..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUOZWP_e3BzX",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the data\n",
        "\n",
        "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmvqgnJzHoWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Done..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjZP-rnu7h1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XisUFm5-3RCl",
        "colab_type": "text"
      },
      "source": [
        "### Plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGExN0GN3Vb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6NRNM8T3dob",
        "colab_type": "text"
      },
      "source": [
        "### Build the Model\n",
        "\n",
        "Without convolutional layer: (we give an example here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L31HaqMCAOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# No Convolutional Layer \n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njdJtPABDQhP",
        "colab_type": "text"
      },
      "source": [
        "With convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJtPJgwHtsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with convolutional layers\n",
        "# your turn (you can ask google to help you)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsV0SXz2DmH9",
        "colab_type": "text"
      },
      "source": [
        "Compare the learning curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznfoqIEDrzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}