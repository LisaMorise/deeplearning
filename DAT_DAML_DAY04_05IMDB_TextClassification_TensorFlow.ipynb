{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAT_DAML_DAY04_05IMDB_TextClassification_TensorFlow.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Fv-jfQvJp2U9",
        "GuIXtU-BBmD-",
        "e5NskLmMBcIJ",
        "b47HT3pK2dRN",
        "E9TreZX3GYGA",
        "luUl1fE7KyoV",
        "xKnz-N5jTaBD",
        "3HNDQCnlT3D2",
        "cHhUp-TnWq93",
        "GODrqUJ1XmlW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gndE8JY1wQfT",
        "colab_type": "text"
      },
      "source": [
        "<table ><tr><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://web.facebook.com/DAT.KUSRC/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1dNBiKikzW1-osi6lleLOgSOKQ65IIfMC\" height=\"50px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://www.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ZfGOBmxAwg8SAhyseFziyinzxBGme78a\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "<a href=\"https://www.tensorflow.org/\" target=\"_blank\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/1200px-TensorFlowLogo.svg.png\" height=\"80px\"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "</td><td valign='center' bgcolor='white'>\n",
        "  <a href=\"https://mike.cpe.ku.ac.th/\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1s6r3iG_Slpu_NSWqdt5zBp8Z9hV0-zh6\" height=\"50px\"></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrlawtB_A6_p",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "<center><h1>Text Classification with TensorFlow: Movie reviews</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "* Credit: https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOFFyi8ohMDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Text Classification with TensorFlow: Movie reviews...')\n",
        "print('  Brought to you by K.Toto@MikeLab.Net')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pec739WSBF1U",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hKnr1jbAxAQ",
        "colab_type": "text"
      },
      "source": [
        "This notebook classifies movie reviews as <font color=ff00ff>positive</font> or <font color=ff00ff>negative</font> using the text of the review. This is an example of <font color=ff00ff>binary</font> — or two-class — classification problem, an important and widely applicable kind of machine learning problem.\n",
        "\n",
        "We will use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). The data has been split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are <font color=ff00ff>balanced</font>, meaning they contain an equal number of positive and negative reviews. \n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv-jfQvJp2U9",
        "colab_type": "text"
      },
      "source": [
        "## Idea behind the text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6cl8e6013Jq",
        "colab_type": "text"
      },
      "source": [
        "Text classification has some unique challenges. So before we get coding, let step through some of them.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1E_Zjj2bon6pf05USkdpzrRcptzdodtk3\" height=\"300px\"></center>\n",
        "\n",
        "First of all, neural networks typically deal with <font color=ff00ff>numbers</font>, not with texts when the learning patterns can be used for prediction of classification. So in this case, we are looking at learning from the movie reviews to see if those reviews are positive or negative. The first step is then to change the words into numbers that represent them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6maPnhyrz2R",
        "colab_type": "text"
      },
      "source": [
        "There will be a little bit more processing of these words into vectors determining their sentiments.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1LY8QqggesnWKrRxON1eJ2SBa15yvId-8\" height=\"270px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YViWTQlK2HDC",
        "colab_type": "text"
      },
      "source": [
        "We will build a deep neuron network model, and then feed the movie reviews into it, and let the model infers the answer whether those reviews are positive or negative sentiment.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=15tw_npTw52qa2jkCUWM9gcv5kqBA5dln\" height=\"400px\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuIXtU-BBmD-",
        "colab_type": "text"
      },
      "source": [
        "## Import and check the Tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQAGQYT60-M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' OLD Tensorflow import code that will be deleted soon\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWZfenux3Q8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install -q tensorflow-hub\n",
        "!pip install -q tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Keras version: \", keras.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NskLmMBcIJ",
        "colab_type": "text"
      },
      "source": [
        "## Download the IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zbejcgzARIh",
        "colab_type": "text"
      },
      "source": [
        "The IMDB dataset comes packaged with Tensorflow. It has already been prepocessed such that the reviews (sequence of words) have been converted to sequence of integers, where each integer represents a specific word in a dictionary. \n",
        "\n",
        "In that dictionary, the frequent used or common words come first in integer number, while the less common ones come higher integer number later.\n",
        "\n",
        "The following code downloads the IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnx85Pps1Sv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xvFYaYeCT8L",
        "colab_type": "text"
      },
      "source": [
        "The argument `num_words=10000` keeps only the top 10,000 most frequently occuring words in the training data, i.e. the top 10,000 words that are used across all the reviews. The rare words are discards to keep the size of the data manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEoNSPGRQ4m5",
        "colab_type": "text"
      },
      "source": [
        "After loading, we have our <font color=ff00ff>training</font> data and labels, as well as as our <font color=ff00ff>test</font> data and labels.\n",
        "\n",
        "First we will look at our training data. We can see that we have a total of 25,000 items of data and 25,000 labels describing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hqpiLVUTyfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_data), len(train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b47HT3pK2dRN",
        "colab_type": "text"
      },
      "source": [
        "## Explore the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3awfgX_tP5",
        "colab_type": "text"
      },
      "source": [
        "The dataset comes preprocessed; each example is an array of integers representing the words of the movie review. \n",
        "\n",
        "Each label is an integer value of either 0 or 1, where <font color=ff00ff>0</font> indicate the <font color=ff00ff>negative</font> review, and <font color=ff00ff>1</font> the <font color=ff00ff>positive</font> review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ttU5aR1hlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Training entries: {len(train_data)}, labels: {len(train_label)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp8t-dVu133N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "  if train_label[i] == 1:\n",
        "    print('pos: ', end='')\n",
        "  else:\n",
        "    print('neg: ', end='')\n",
        "  print(train_data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JqNiVUA-tQK",
        "colab_type": "text"
      },
      "source": [
        "Each of the reviews has been indexed into the <font color=ff00ff>array of words</font>. A review will start with the word indexed with the integer number <font color=00ffff>**1**</font> indicating the <font color=ffff00>\\<start\\></font> of the review. For eample, in the first review the index number <font color=00ffff>**14**</font> and <font color=00ffff>**22**</font> correspond to the word <font color=ffff00>**this**</font> and <font color=ffff00>**film**</font>. The length of the reviews does vary. For example, the length of the first and the second review are as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnuMeN_O174-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59L7lO12lfC",
        "colab_type": "text"
      },
      "source": [
        "### Convert the integers back to words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh_3SDCWFmcT",
        "colab_type": "text"
      },
      "source": [
        "It may useful to know how to convert integers back to text. Here, we'll create a helpful function to query a dictionary object that contains the integer of string mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTWuKY2g2EoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "for k,v in word_index.items():\n",
        "  if v <= 4:\n",
        "    print(f'{k}: {v} ', end=' ')\n",
        "print('| this:', word_index['this'], ' film:', word_index['film'])\n",
        "\n",
        "# the first indices are reserved\n",
        "# see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset/44891281\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index['<PAD>'] = 0\n",
        "word_index['<START>'] = 1\n",
        "word_index['<UNK>'] = 2 # unknown\n",
        "word_index['<UNUSED>'] = 3\n",
        "\n",
        "for k,v in word_index.items():\n",
        "  if v <= 4:\n",
        "    print(f'{k}: {v} ', end=' ')\n",
        "print('| this:', word_index['this'], ' film:', word_index['film'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR06qN_b6Ce5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "for i in range(10):\n",
        "  if train_label[i] == 1:\n",
        "    print('pos: ', end='')\n",
        "  else:\n",
        "    print('neg: ', end='')\n",
        "  print(decode_review(train_data[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfOvbr7xWcjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9TreZX3GYGA",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K48RCpPmGku-",
        "colab_type": "text"
      },
      "source": [
        "The reviews --<font color=ff00ff>the arrays of integers</font>-- must be converted to <font color=00ffff>tensors</font> before fed into the neural network. This conversion can be done a couple of ways:\n",
        "*   <font color=ffff00>One-hot-encode</font> the arrays to convert them into vectors of 0s and 1s. For example, the sequence [14, 22] would become a 10,000-dimensional vector that is all zeros except for the indices 14 and 22, which are ones. Then, make this the first layer in our network --a <font color=ff00ff>Dense layer</font>-- that can handle floating point vector data. This approach is memory intensive, though, requring a `num_words * num_reviews` size matrix. \n",
        "*   Alternatively, we can <font color=ffff00>pad the arrays</font> so they all have the same length tensor of shape `num_examples * max_length`. We can use an <font color=ff00ff>embedding layer</font> capable of handling this shape as the first layer in our network.\n",
        "\n",
        "Let's look back to the length of the first two movie reviews, again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odQO5REmzRkE",
        "colab": {}
      },
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi3yX_vXzHLy",
        "colab_type": "text"
      },
      "source": [
        "We here will go with the second approach. Since the movie reviews must be the same length, we will use the pad_sequence( ) function to standardize the lengths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoUNBTAR21iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
        "                                                        value=word_index['<PAD>'], \n",
        "                                                        padding='post', \n",
        "                                                        maxlen=256)\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, \n",
        "                                                       value=word_index['<PAD>'], \n",
        "                                                       padding='post', \n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuyxwfQnJ730",
        "colab_type": "text"
      },
      "source": [
        "Now, let's look at the length of the examples after padding now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD0B1lmwJjad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuRVsegwKLjV",
        "colab_type": "text"
      },
      "source": [
        "And inspect the (now padded) first review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB4xdrbC27Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk0CkMeEZ02M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(decode_review(train_data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luUl1fE7KyoV",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2naScP-LAMx",
        "colab_type": "text"
      },
      "source": [
        "The neural network is created by <font color=ffff00>stacking the layers</font> -- this requires two main architectural decisions:\n",
        "*   How many <font color=ffff00>*layers*</font> to use in the model?\n",
        "*   How many <font color=ffff00>*hidden units*</font> to use for each layer?\n",
        "\n",
        "In this example, the input data consists of an array of word-indices. The labels to predict are either 0 or 1. Let's build a model for this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpgWU5vSKTtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input shape is the vocabualry count used for the movie reviews (10,000 words)\n",
        "vocab_size = 10000\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYjCTmjOM3nc",
        "colab_type": "text"
      },
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "1.   The first layer is an <font color=00ffff>`Embedding`</font> layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (`batch`, `sequence`, `embedding`).\n",
        "2.   Next, the <font color=00ffff>`GlobalAveragePooling1D`</font> layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model can handle input of variable length, in the simplest way possible.\n",
        "3.   This fixed-length outout vector is piped through a fully-connected (<font color=00ffff>`Dense`</font>) layer with 16 hidden units.\n",
        "4.   The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confident level.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8pC6pNCP0GZ",
        "colab_type": "text"
      },
      "source": [
        "**Hidden units**\n",
        "\n",
        "The above model has two intermediate or 'hidden' layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space of the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
        "\n",
        "If a model has more hidden units (a higher-dimensional represntation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computational expensive and may lead to learning unwanted patterns -- patterns that improve performance on training data by not on the test data, which is called <font color=ff00ff>*overfitting*</font>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VL8uShrRZhB",
        "colab_type": "text"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "A model needs a <font color=ff00ff>loss function</font> and an <font color=ff00ff>optimizer</font> for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the <font color=ffff00>`binary_crossentropy`</font> loss function. \n",
        "\n",
        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5nHkY5tMXle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKnz-N5jTaBD",
        "colab_type": "text"
      },
      "source": [
        "## Create a validation set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58GrBdg0SnJA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1gfkidJF3a-VxKAjcXzMEcF0Pl3kFyhb5)\n",
        "\n",
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So, we creat a <font color=ff00ff>*validation* set</font> by setting apart 10,000 examples from the original 25,000 samples of the training data. \n",
        "\n",
        "\n",
        "Why not use the <font color=ff00ff>testing set</font> now? Our goal is to develop and tune our model using only the training data, and then use the test data just once to evaluate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTLkW5FdShrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_label[:10000]\n",
        "partial_y_train = train_label[10000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HNDQCnlT3D2",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9q3OPZ0T5iV",
        "colab_type": "text"
      },
      "source": [
        "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples for the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV--GjvQUZD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHhUp-TnWq93",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxe6Xl_VW5uj",
        "colab_type": "text"
      },
      "source": [
        "And let's see how the model performs. Two values will be returned. <font color=ff00ff>Loss</font> (a number which represents the error, lower value is better), and <font color=ff00ff>accuracy</font>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vamUMJyAXLPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_data, test_label)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bel6sc9YXWLe",
        "colab_type": "text"
      },
      "source": [
        "We can see that, this fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model could get closer to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODrqUJ1XmlW",
        "colab_type": "text"
      },
      "source": [
        "### Creat a graph of accuracy and loss over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7bK7y5XwDn",
        "colab_type": "text"
      },
      "source": [
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmxdkHFaXRbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0VlFR11YP24",
        "colab_type": "text"
      },
      "source": [
        "There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for caamparison, as well as the training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fERWuBxUYIxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4j2E-laDOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.clf() # clear figure\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'g', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOyPpOytbcAY",
        "colab_type": "text"
      },
      "source": [
        "In this plot, the blue dots represent the training loss and accuracy, and the solid green lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization --it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy -- they seem to peak after about fifteen to twenty epochs. This is an example of overfitting: the model performs better on the training data than it does it on the data it has never seen before. After this point, the model over-optimizes and learns representations <font color=ff00ff>*specific*</font> to the training data that do not <font color=ff00ff>*generalize*</font> to test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSXSyVCZftyP",
        "colab_type": "text"
      },
      "source": [
        "For this particular case, we could prevent overfitting by simply stopping the training after fifteen or twenty or so epochs. Here, we can do this automatically with a callback (let the reader to explore!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no4u4PIugCly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's also test it against a couple of planted reviews.\n",
        "# One of them is a bunch of random words, and the other\n",
        "# is the biased review we created.\n",
        "\n",
        "rand_review = np.random.randint(10000, size=256)\n",
        "biased_review = np.full(256, 530) # 530 is the word 'brilliant'\n",
        "test_data = np.append(test_data, [rand_review], axis=0)\n",
        "test_data = np.append(test_data, [biased_review], axis=0)\n",
        "\n",
        "# And here is the prediction.\n",
        "# It's an array of predictions for the test data at the particular index,\n",
        "# so let's manually look at the first few, as well as the ones we planted\n",
        "# to the end.\n",
        "print(model.predict(test_data))\n",
        "print(decode_review(test_data[-2]))\n",
        "print(decode_review(test_data[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}